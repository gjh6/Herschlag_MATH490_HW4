{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Evaluating Evidence\n",
    "### Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: Underlying Sampling Algorithm (Metropolis-Hastings)\n",
    "\n",
    "Define a prior, $\\pi(\\theta) \\propto \\chi_{[-5,5]^d}(\\theta)$ be a multi-dimenstional density on $\\theta\\in\\mathbb{R}^d$.\n",
    "\n",
    "Consider the likelihood function \n",
    "\n",
    "$L(\\theta) = \\prod_{i=1}^d\\exp\\big(-\\frac{\\theta_i^2}{2v^2}\\big)$\n",
    "\n",
    "with $v = 0.1$\n",
    "\n",
    "Throughout this homework, set $d=20$ (i.e. moderately high)\n",
    "\n",
    "**Problem 1** Design an MCMC scheme using Metropolis-Hastings that can takin in an (unnormalized) probability density, $\\mu$, a stepsize, $h$, and a number of steps, $n$, an initial condition, $x_0\\in \\mathbb{R}^d$, and returns the final random state in the chain, $x_n$.  Let the proposal in the chain, propose a new state with distribution $\\mathcal{N}(x, 2h I)$, where $I$ is the $d\\times d$ identity matrix.\n",
    "\n",
    "**Problem 2** Test your algorithm with $\\mu = L$ and plot the radial density of $x$ for a collection of outputs (can do a single run with multiple outputs or multiple runs with one output, or something in between).  Develope an analytic expression or approximation and compare your numerical result with the analytic result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: Importance Sampling (Harmonic Mean)\n",
    "\n",
    "**Problem 3** Use importance sample to have your MCMC algorithm (from problem 1) sample $\\pi$ for $10^5$ steps; use the $10^5$ outputs to perform importance sampling to estimate the normalization constant of $L(\\theta)\\pi(\\theta)$.  Approximate the analytic value of $\\int L(\\theta)\\pi(\\theta) d\\theta$ and compare your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3: Thermodynamic Inetgration\n",
    "\n",
    "**Problem 4** Use thermodynamic integration to have your MCMC algorithm (from problem 1) sample $L^\\beta\\pi$.  Sample from a single chain, in which you take 1000 steps (samples) for 100 different values of $\\beta = \\{i/99\\}_{i=0}^{99}$.  Do this by\n",
    "\n",
    "- Start with $i=0$ and take 1000 samples to estimate $\\mathbb{E}_{\\beta}(L(\\theta))$.\n",
    "- Increment $i$ by one and repeat until $\\beta = 1$.  \n",
    "\n",
    "Use this to approximate $Z = \\int L\\pi d\\theta$.\n",
    "\n",
    "Repeat this proceedure by starting $i=99$ and decreasing $i$ until $\\beta = 0$.\n",
    "\n",
    "Compare both answers to your approximate analytic value.\n",
    "\n",
    "*Note* Your answer may depend on your steps size.  You should tune $h$ so that you propose larger moves but that you don't reject the moves too frequently.\n",
    "\n",
    "*Hint:* It may be useful to use a function factory here to generate (unnormalized) distributions to pass to your MCMC algorithm (see example code below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def L(theta):\n",
    "    return theta\n",
    "\n",
    "def pi(theta):\n",
    "    return 2*theta\n",
    "\n",
    "def temperedMeasure(likelihood, prior, beta):\n",
    "    def temperedPosterior(theta):\n",
    "        return (likelihood(theta)**beta)*prior(theta)\n",
    "    return temperedPosterior\n",
    "\n",
    "posterior = temperedMeasure(L, pi, 1)\n",
    "print(posterior(3))\n",
    "\n",
    "def constrainedMeasure(likelihood, prior, lmbd):\n",
    "    def constrainedPosterior(theta):\n",
    "        return prior(theta) if likelihood(theta) > lmbd else 0\n",
    "    return constrainedPosterior\n",
    "\n",
    "posterior = constrainedMeasure(L, pi, 3)\n",
    "print(posterior(4))\n",
    "print(posterior(2))\n",
    "\n",
    "# then: metropolisHastings(distribution = posterior, steps = 1, h = 0.1, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4: Nested Sampling\n",
    "\n",
    "**Problem 4** Start with 100 samples from $\\pi$ and use nested sampling to estimate $Z$.  When you do this, take the deleted point, re-initialize it at one of the 99 random remaining points, and evolve it 1000 steps according to your Metropolis-Hastings scheme with a restricted threshold on the likelihood.\n",
    "\n",
    "Again, compare your estimated value to the analytic value.\n",
    "\n",
    "**Problem 5** Plot your estimate values of $\\log(Z)$ of all four methods against the analytic value.\n",
    "\n",
    "*Note* Since the volume shinks with the likelihood, you should think about dynamically reducing $h$ as the likelihood threshold grows; only do this if you're getting stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Phase transitions\n",
    "\n",
    "Repeat the above proceedures with \n",
    "\n",
    "$L(\\theta) = \\prod_{i=1}^d\\exp\\big(-\\frac{\\theta_i^2}{2v^2}\\big) + \\prod_{i=1}^d\\exp\\big(-\\frac{\\theta_i^2}{2u^2}\\big)$\n",
    "\n",
    "with $v = 0.1$ and $u = 0.01$. Note this function has a \"phase transition\" in that when $\\beta$ is larger there is a deep well near zero that will have high probability, and when $\\beta$ is small enough, we will likely escape this well.\n",
    "\n",
    "Again determine the normalization constant (using your code above)\n",
    "\n",
    "**Problem 6:** Analytically (approximately)  \n",
    "**Problem 7:** Via importance sampling with $10^5$ samples per estimate  \n",
    "**Problem 8:** Via thermodynamic integration with 100 values of $\\beta$ and 1000 samples from each $L^\\beta \\pi$ (per estimate; with $\\beta$ increasing and decreasing)  \n",
    "**Problem 9:** Via nested sampling with 100 samples and 1000 steps on the chain to de-correlate new draws    \n",
    "**Problem 10:** Analyze the accuracy of your estimates by comparing with the analytic approximation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "\n",
    "Show that if $\\theta\\sim\\pi$, and $\\lambda = L(\\theta)$, that $X(\\lambda) = \\int_{L(y)>\\lambda} \\pi(y) dy$ is uniformly distributed on $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
